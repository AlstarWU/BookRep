{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 第7章·实验4：面向时序分析的 RNN（LSTM/GRU）",
        "",
        "本 Notebook 基于同一合成时序数据集，实现循环神经网络对序列的分类，涵盖数据预处理、模型搭建、训练评估与实验拓展，方便按照课件要求完成实验记录。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 环境准备",
        "与 CNN 版本保持一致：`torch`, `numpy`, `matplotlib`, `scikit-learn`。若有 GPU 会自动切换。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设定随机种子\n",
        "torch.manual_seed(7)\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 数据集与 DataLoader",
        "- 仍使用正弦波 vs. 方波分类示例，方便对比 CNN 与 RNN。",
        "- 将序列转置为 (序列长度, 特征数) 以便喂入 RNN。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def generate_wave(sample_len: int, kind: str, noise_scale: float = 0.15) -> np.ndarray:\n",
        "    x = np.linspace(0, 2 * math.pi, sample_len)\n",
        "    if kind == \"sine\":\n",
        "        base = np.sin(x)\n",
        "    elif kind == \"square\":\n",
        "        base = np.sign(np.sin(x))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported kind\")\n",
        "    noise = np.random.normal(scale=noise_scale, size=sample_len)\n",
        "    return base + noise\n",
        "\n",
        "def build_dataset(n_samples: int = 1200, sample_len: int = 200) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    signals = []\n",
        "    labels = []\n",
        "    for _ in range(n_samples // 2):\n",
        "        signals.append(generate_wave(sample_len, \"sine\"))\n",
        "        labels.append(0)\n",
        "        signals.append(generate_wave(sample_len, \"square\"))\n",
        "        labels.append(1)\n",
        "    signals = np.stack(signals).astype(np.float32)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "    max_val = np.abs(signals).max()\n",
        "    signals = signals / max_val\n",
        "    return signals, labels\n",
        "\n",
        "class WaveformSequence(Dataset):\n",
        "    def __init__(self, signals: np.ndarray, labels: np.ndarray):\n",
        "        self.signals = torch.tensor(signals)\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # RNN 期望形状: (seq_len, feature_dim)，这里 feature_dim=1\n",
        "        seq = self.signals[idx].unsqueeze(-1)\n",
        "        return seq, self.labels[idx]\n",
        "\n",
        "signals, labels = build_dataset(n_samples=1200, sample_len=200)\n",
        "indices = np.random.permutation(len(labels))\n",
        "train_end = int(len(labels) * 0.7)\n",
        "val_end = int(len(labels) * 0.85)\n",
        "train_idx, val_idx, test_idx = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
        "\n",
        "train_data = WaveformSequence(signals[train_idx], labels[train_idx])\n",
        "val_data = WaveformSequence(signals[val_idx], labels[val_idx])\n",
        "test_data = WaveformSequence(signals[test_idx], labels[test_idx])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "len(train_data), len(val_data), len(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 序列可视化",
        "随机展示两类序列，观察波形和噪声水平，便于后续调参。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "samples_to_plot = 3\n",
        "fig, axes = plt.subplots(samples_to_plot, 2, figsize=(10, 6), sharex=True, sharey=True)\n",
        "for row in range(samples_to_plot):\n",
        "    for col, label in enumerate([0, 1]):\n",
        "        idx = (labels == label).nonzero()[0][row]\n",
        "        axes[row, col].plot(signals[idx])\n",
        "        axes[row, col].set_title(f\"label={label}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 构建循环神经网络",
        "示例使用双层 LSTM + 注意力式加权平均（通过可学习的线性层近似），并提供 GRU 的替换示例。可按照课件实验要求调整隐藏维度、层数或改用双向 RNN。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, num_classes=2, bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
        "        direction_factor = 2 if bidirectional else 1\n",
        "        self.attention = nn.Linear(hidden_dim * direction_factor, 1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * direction_factor, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, feature_dim)\n",
        "        outputs, _ = self.lstm(x)\n",
        "        # 简单注意力权重\n",
        "        weights = torch.softmax(self.attention(outputs).squeeze(-1), dim=1)  # (batch, seq_len)\n",
        "        context = torch.sum(outputs * weights.unsqueeze(-1), dim=1)  # 加权和\n",
        "        return self.classifier(context)\n",
        "\n",
        "# 若想改用 GRU：将上方 self.lstm = nn.GRU(...) 即可\n",
        "model = LSTMClassifier().to(device)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 训练与验证",
        "- 优化器使用 Adam，损失函数为交叉熵。",
        "- 记录训练/验证的损失与准确率，便于对比 CNN 的收敛速度与泛化表现。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += loss.item() * y.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def run_training(model, train_loader, val_loader, epochs=20, lr=1e-3):\n",
        "    history = defaultdict(list)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        print(f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
        "              f\"train_acc={train_acc:.3f} val_acc={val_acc:.3f}\")\n",
        "    return history\n",
        "\n",
        "history = run_training(model, train_loader, val_loader, epochs=20, lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 学习曲线",
        "与 CNN 结果对照，观察 RNN/LSTM 的收敛特性和泛化趋势。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].plot(history[\"train_loss\"], label=\"train\")\n",
        "axes[0].plot(history[\"val_loss\"], label=\"val\")\n",
        "axes[0].set_title(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[1].plot(history[\"train_acc\"], label=\"train\")\n",
        "axes[1].plot(history[\"val_acc\"], label=\"val\")\n",
        "axes[1].set_title(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 测试集评估",
        "输出分类报告与混淆矩阵，为实验报告提供量化指标。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.3f}\")\n",
        "\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(y.numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"sine\", \"square\"]))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "im = ax.imshow(cm, cmap=\"Greens\")\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels([\"sine\", \"square\"])\n",
        "ax.set_yticklabels([\"sine\", \"square\"])\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "for (i, j), val in np.ndenumerate(cm):\n",
        "    ax.text(j, i, int(val), ha=\"center\", va=\"center\", color=\"black\")\n",
        "fig.colorbar(im)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 课后/实验要求提示",
        "- **结构对比**：尝试 GRU、单层/多层、双向与否，并记录验证集结果。",
        "- **序列截断/填充**：调整序列长度或使用滑动窗口，讨论对性能的影响。",
        "- **训练策略**：尝试调节学习率、梯度裁剪或加入权重衰减，观察梯度稳定性。",
        "- **结果总结**：结合 CNN 实验，对比两类模型的优势与局限，给出改进建议。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}